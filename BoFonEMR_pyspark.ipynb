{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of BoF with pyspark on EMR, loading from and storing on S3   \n",
    "\n",
    "Here we show an example on how to load data from S3, here images, how to compute features of images with a MapReduce mindset, and how to store the dataframe containing the bag-of-feature on the S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6898f5d284a4139ad489cf479b69556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1591068852364_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-46-223.ap-northeast-2.compute.internal:20888/proxy/application_1591068852364_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-37-224.ap-northeast-2.compute.internal:8042/node/containerlogs/container_1591068852364_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.0.4\n",
      "  Using cached pandas-1.0.4-cp36-cp36m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.6/site-packages (from pandas==1.0.4) (1.14.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas==1.0.4) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas==1.0.4) (1.13.0)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.0.4 python-dateutil-2.8.1\n",
      "\n",
      "Requirement already satisfied: numpy in /usr/local/lib64/python3.6/site-packages (1.14.5)\n",
      "\n",
      "Collecting Pillow\n",
      "  Using cached Pillow-7.1.2-cp36-cp36m-manylinux1_x86_64.whl (2.1 MB)\n",
      "Installing collected packages: Pillow\n",
      "Successfully installed Pillow-7.1.2\n",
      "\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.2.0.34-cp36-cp36m-manylinux1_x86_64.whl (28.2 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib64/python3.6/site-packages (from opencv-python) (1.14.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.2.0.34\n",
      "\n",
      "Collecting python-resize-image\n",
      "  Using cached python_resize_image-1.1.19-py2.py3-none-any.whl (8.4 kB)\n",
      "Collecting requests>=2.19.1\n",
      "  Using cached requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: Pillow>=5.1.0 in /mnt/tmp/1591078787071-0/lib64/python3.6/site-packages (from python-resize-image) (7.1.2)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2020.4.5.1-py2.py3-none-any.whl (157 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Installing collected packages: urllib3, idna, certifi, chardet, requests, python-resize-image\n",
      "Successfully installed certifi-2020.4.5.1 chardet-3.0.4 idna-2.9 python-resize-image-1.1.19 requests-2.23.0 urllib3-1.25.9\n",
      "\n",
      "Processing /mnt/var/lib/livy/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c/sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-0.15.1-py3-none-any.whl (298 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.6/site-packages (from scikit-learn->sklearn) (1.14.5)\n",
      "Collecting scipy>=0.19.1\n",
      "  Using cached scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: joblib, scipy, threadpoolctl, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.15.1 scikit-learn-0.23.1 scipy-1.4.1 sklearn-0.0 threadpoolctl-2.1.0\n",
      "\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.13.20-py2.py3-none-any.whl (128 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting botocore<1.17.0,>=1.16.20\n",
      "  Using cached botocore-1.16.20-py2.py3-none-any.whl (6.2 MB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/site-packages (from boto3) (0.9.4)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /mnt/tmp/1591078787071-0/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.20->boto3) (1.25.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1591078787071-0/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.20->boto3) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.20->boto3) (1.13.0)\n",
      "Installing collected packages: docutils, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.13.20 botocore-1.16.20 docutils-0.15.2 s3transfer-0.3.3\n",
      "\n",
      "Collecting s3fs\n",
      "  Using cached s3fs-0.4.2-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /mnt/tmp/1591078787071-0/lib/python3.6/site-packages (from s3fs) (1.16.20)\n",
      "Collecting fsspec>=0.6.0\n",
      "  Using cached fsspec-0.7.4-py3-none-any.whl (75 kB)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /mnt/tmp/1591078787071-0/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.15.2)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /mnt/tmp/1591078787071-0/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (1.25.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1591078787071-0/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (2.8.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.13.0)\n",
      "Installing collected packages: fsspec, s3fs\n",
      "Successfully installed fsspec-0.7.4 s3fs-0.4.2"
     ]
    }
   ],
   "source": [
    "# install python librairies on the nodes\n",
    "sc.install_pypi_package(\"pandas==1.0.4\")\n",
    "sc.install_pypi_package(\"s3fs\")\n",
    "sc.install_pypi_package(\"Pillow\")\n",
    "sc.install_pypi_package(\"python-resize-image\")\n",
    "sc.install_pypi_package(\"opencv-python\")\n",
    "sc.install_pypi_package(\"boto3\")\n",
    "\n",
    "#import python librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "from PIL import Image, ImageOps\n",
    "from resizeimage import resizeimage\n",
    "import cv2\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Images are treated and resized and then ORB algorithm is used to extract the features in images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344bc4c27e74438ebc338de6f86496c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Apple Braeburn', array([253, 245, 253, 231, 188, 111,  95, 188, 127, 242, 191,  28, 219,\n",
      "       255,  89,  59, 191, 215, 125, 251, 249, 103, 251, 186, 251, 255,\n",
      "       229,  60, 232, 251, 198, 114], dtype=uint8))"
     ]
    }
   ],
   "source": [
    "s3_fs = s3fs.S3FileSystem()\n",
    "image_path_list = s3_fs.ls('s3://XXXXXXXXXXXXX') # hidding the name of the bucket in the S3 for this public notebook\n",
    "image_path_ini= ''\n",
    "\n",
    "S3_path = 's3://XXXXXXXXXX' # hidding the name of the bucket in t the S3 for this public notebook\n",
    "for img_pth in image_path_list:\n",
    "    imgs_name = (img_pth.split(\"/\")[2])\n",
    "    image_path_ini += (S3_path + imgs_name + ',')\n",
    "image_path = image_path_ini[:-1]\n",
    "\n",
    "def img2catNdesc(image):\n",
    "    try:\n",
    "        # From bytes to image\n",
    "        name, img = image\n",
    "        image_img = Image.open(io.BytesIO(img))\n",
    "        category = name.split('/')[-2]\n",
    "        \n",
    "        # Rescale image\n",
    "        width = 100\n",
    "        img_equ = ImageOps.equalize(image_img)\n",
    "        img_full = resizeimage.resize_cover(img_equ, [width, width])\n",
    "        arr_full = np.array(img_full)\n",
    "        orbal = cv2.ORB_create(nfeatures = 50)\n",
    "        kp, des = orbal.detectAndCompute(arr_full,None)\n",
    "        tup_cat_des = ()\n",
    "        cat_exp =''\n",
    "        des_exp = []\n",
    "        # initially one image gives one category and zero to several descriptors\n",
    "        # the for loop generates tuples to set one cat per descriptor\n",
    "        for ides in des:\n",
    "            tup_cat_des = (category, ides)\n",
    "        cat_exp, des_exp = tup_cat_des        \n",
    "    except:\n",
    "        arr_full = None\n",
    "    return cat_exp, des_exp\n",
    "\n",
    "# Takes all images and names\n",
    "images =sc.binaryFiles(image_path, minPartitions = 4)\n",
    "img_f1 = images.map(lambda img: img2catNdesc(img))\n",
    "img_coll = img_f1.collect()\n",
    "\n",
    "# example of the first tuple\n",
    "img_coll[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2da7eba6edf4617b5062792af3bbc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0    1    2    3    4    5  ...   27   28   29   30   31   32\n",
      "0  Apple Braeburn  253  245  253  231  188  ...  229   60  232  251  198  114\n",
      "1  Apple Braeburn  253  245  253  247  188  ...  229   60  252  251  198  114\n",
      "2  Apple Braeburn  253  253  253  247  172  ...  229   52  232  251  198  114\n",
      "3  Apple Braeburn  253  253  253  247  172  ...  229   52  233  251  198  114\n",
      "4  Apple Braeburn  191   95  105  235  244  ...  167  196  191  191  208  241\n",
      "\n",
      "[5 rows x 33 columns]"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data=img_coll, columns=['cate','descrip'])\n",
    "cat_only = df[['cate']]\n",
    "\n",
    "# changing list of array to features of df for each value in an array of the list\n",
    "df_des_cells = pd.DataFrame(df['descrip'].values.tolist())\n",
    "df_cat_des_cells = pd.concat([cat_only,df_des_cells],axis =1,ignore_index=True).dropna()\n",
    "df_cat_des_cells.columns = df_cat_des_cells.columns.astype(str)\n",
    "df_cat_des_cells.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b8f986d7e6412ab4a872039de293ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+\n",
      "|              cate|               words|                 bof|\n",
      "+------------------+--------------------+--------------------+\n",
      "|    Apple Braeburn|[13, 16, 13, 3, 8...|(20,[0,3,5,6,8,9,...|\n",
      "|Apple Crimson Snow|[5, 10, 10, 10, 1...|(20,[0,1,2,3,4,7,...|\n",
      "+------------------+--------------------+--------------------+"
     ]
    }
   ],
   "source": [
    "# moving to pyspark dataframe\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "spark_df = sqlContext.createDataFrame(df_cat_des_cells)\n",
    "\n",
    "# vector assemblor geneates a column called \"features\" in the pyspark df and storing an array of the feature\n",
    "# the \"features\" column will be used as input of the pyspark KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "input_list = df_cat_des_cells.columns.tolist()\n",
    "# removing in the list, the column name 0 containing the categories \n",
    "# before assemblying the other feature values into an array in the column \"features\"\n",
    "del input_list[0]\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=input_list, outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(spark_df)\n",
    "\n",
    "# clustering with 20 clusters which will be the references features for the bag of features (bof)\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=20, seed=1)  # choose 20 clusters \n",
    "model = kmeans.fit(new_df.select(\"features\"))\n",
    "\n",
    "# transformed variable is an input df with a new column called prediction (created and named by pyspark KMeans)\n",
    "transformed = model.transform(new_df)\n",
    "\n",
    "#CountVectorizer has a list of features as an input ex :[1, 9, 6, 6, 9, 8]\n",
    "# first we take only the category column called \"0\" and \"prediction\" generate a rdd \n",
    "#then reducebykey to generate tuples containing the category and the features numbers for each category\n",
    "rdd_features = transformed.select('0', 'prediction').rdd.map(lambda x:x).reduceByKey(lambda y,z: str(y) + ',' + str(z))\n",
    "\n",
    "# reshaping the tuple as a tuple of a category and a list of features' numbers \n",
    "# then reducing to a dataframe\n",
    "df_cat_feat= rdd_features.map(lambda tup: (tup[0], str(tup[1]).split(','))).toDF(['cate','words'])\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "# the dataframe column was named \"words\" on purpose since it is defined  as an input value in Countvectorizer\n",
    "# Countvectorizer will comptue the bag of features based on the arrays in the column \"words\"\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"bof\")\n",
    "cv_fitted =cv.fit(df_cat_feat)\n",
    "df_cat_bof =cv_fitted.transform(df_cat_feat)\n",
    "df_cat_bof.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61da88fff9dc4afa9d54ed4cda66185a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '9F4D8F7691F2F22B', 'HostId': 'ZhnYOWcQukqiCls204Qvnzpszc9ylR7CexjWQyDj+LzdKcaQgfvGirAnEhB4j+cJ/3YsRnnI9dQ=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'ZhnYOWcQukqiCls204Qvnzpszc9ylR7CexjWQyDj+LzdKcaQgfvGirAnEhB4j+cJ/3YsRnnI9dQ=', 'x-amz-request-id': '9F4D8F7691F2F22B', 'date': 'Tue, 02 Jun 2020 06:24:18 GMT', 'etag': '\"fca300601a9548fca537a579b23ada9e\"', 'content-length': '0', 'server': 'AmazonS3'}, 'RetryAttempts': 1}, 'ETag': '\"fca300601a9548fca537a579b23ada9e\"'}"
     ]
    }
   ],
   "source": [
    "# changing to pandas df before storing\n",
    "df_cat_bof_pd = df_cat_bof.toPandas()\n",
    "\n",
    "from io import StringIO\n",
    "import boto3\n",
    "bucket = 'XXXXXXX' # hidding the name of the bucket for this public notebook\n",
    "csv_buffer = StringIO()\n",
    "df_cat_bof_pd.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'df_cat_bof_pd_fromEMR.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Furework : Compute the bag of features with a CNN and perform clustering of the BoF computed by the ORB algorithm and the CNN.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
